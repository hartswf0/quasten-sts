# A Spencer-Brown Inspired Ontological Model of Forsythe's "Engineering Knowledge"

This report presents a structured ontological index of Diana E. Forsythe's influential 1993 work "Engineering Knowledge: The Construction of Knowledge in Artificial Intelligence" using Spencer-Brown's calculus of distinctions. By marking entities as distinctions (◻) and relationships as indications (→), we create a formal representation that illuminates the conceptual architecture of how knowledge is constructed in artificial intelligence.

## Foundational Entities and Primary Distinctions

### Core Theoretical Constructs

◻ **Engineering Knowledge**: Forsythe's 1993 paper examining the construction of knowledge in AI

◻ **knowledge engineering**: Process used to build expert systems through knowledge acquisition

◻ **expert systems**: Computer programs designed to emulate human expertise in decision-making

◻ **knowledge acquisition**: Process of collecting and encoding information for expert systems

◻ **knowledge elicitation**: Face-to-face interviews with experts to extract their knowledge

◻ **anthropological perspective**: Forsythe's methodological approach to studying AI practitioners

◻ **positivist approach**: Epistemological stance of knowledge engineers viewing knowledge as objective

### Key Conceptual Components

◻ **knowledge bottleneck**: Difficulty in transferring expertise from humans to machines

◻ **knowledge cliff**: Metaphor for expert systems' failures when encountering unexpected situations

◻ **tacit assumptions**: Unexamined beliefs held by knowledge engineers

◻ **systematic deletions**: Aspects of knowledge consistently omitted in AI representations

◻ **cultural dimension**: Perspective that technology embodies cultural values and assumptions

## Morphisms and Relational Structures

### Foundational Knowledge Engineering Relationships

◻ **knowledge engineers** → *conduct* → ◻ **knowledge acquisition**
   *Practitioners gather information to build expert systems*

◻ **knowledge acquisition** → *presents* → ◻ **bottleneck**
   *The process is problematic due to complexity of knowledge transfer*

◻ **knowledge elicitation** → *relies on* → ◻ **face-to-face interviews**
   *Engineers interview experts to extract knowledge*

◻ **knowledge engineers** → *perceive* → ◻ **knowledge acquisition as straightforward**
   *Engineers view the process as conceptually uncomplicated*

◻ **knowledge acquisition** → *involves* → ◻ **interpretation and translation**
   *Complex processes transform human knowledge into machine-readable form*

### Epistemological Framework

◻ **knowledge engineers** → *hold* → ◻ **positivist approach**
   *Engineers typically adopt objectivist view of knowledge*

◻ **positivist approach** → *contrasts with* → ◻ **anthropological perspective**
   *Different epistemological foundations of two disciplines*

◻ **tacit assumptions** → *shape* → ◻ **knowledge acquisition procedures**
   *Unexamined beliefs influence how engineers gather information*

◻ **engineers' beliefs** → *are embedded in* → ◻ **resultant technology**
   *Systems embody the assumptions of their creators*

## Complex Conceptual Structures

### Knowledge Cliff Framework

◻ **expert systems** → *tend to* → ◻ **fall off knowledge cliff**
   *Systems fail when encountering situations beyond their programmed knowledge*

◻ **knowledge cliff** → *is attributed to* → ◻ **technological limitations**
   *Engineers view problem as purely technical*

◻ **knowledge cliff** → *is also caused by* → ◻ **tacit assumptions**
   *Forsythe argues that non-technical factors contribute to system failures*

◻ **narrow brittle systems** → *result from* → ◻ **selective knowledge acquisition**
   *Limited representations of expert knowledge lead to limited system capabilities*

### Selective Representation Framework

◻ **knowledge acquisition** → *produces* → ◻ **decontextualized picture**
   *Process yields information removed from its context*

◻ **knowledge representation** → *discounts* → ◻ **real-world practice**
   *Systems exclude much of what experts actually do*

◻ **systematic deletions** → *occur in* → ◻ **engineers' representation of their own work**
   *Engineers selectively define what counts as real work*

◻ **systematic deletions** → *are replicated in* → ◻ **system-building procedures**
   *Same omissions appear in how engineers build systems*

## Cultural Framework of Technology

### Technology's Cultural Dimensions

◻ **technology** → *is often viewed as* → ◻ **value-free**
   *Common perception that technology is neutral*

◻ **anthropological perspective** → *reveals* → ◻ **technology embodies values**
   *Forsythe's analysis shows technology contains cultural assumptions*

◻ **cultural dimension** → *influences* → ◻ **knowledge-based systems**
   *Systems are not value-free but incorporate tacit beliefs*

◻ **expert systems** → *embody* → ◻ **assumptions about knowledge and work**
   *Engineers' beliefs about what counts as knowledge and work are built into systems*

### Alternative Approaches

◻ **AI researchers** → *attempt to solve* → ◻ **knowledge cliff problem**
   *Engineers work on solutions such as embedding common sense*

◻ **CYC project** → *aims to encode* → ◻ **common sense knowledge**
   *Massive effort to create generalized knowledge base*

◻ **engineers** → *focus on* → ◻ **technical solutions**
   *Practitioners seek technical fixes to knowledge representation problems*

◻ **Forsythe's critique** → *points to* → ◻ **non-technical factors**
   *Anthropological analysis identifies cultural and social dimensions*

## Conclusion

This Spencer-Brown inspired ontological model captures the rich theoretical framework of Diana Forsythe's "Engineering Knowledge." Through formal distinctions (◻) and indications (→), we visualize how knowledge is constructed in artificial intelligence through the lens of anthropological analysis. The model highlights Forsythe's central argument: that expert systems embody the tacit assumptions of their creators, particularly regarding what counts as knowledge and work. These assumptions, rather than merely technical limitations, contribute to the systems' brittleness and tendency to "fall off the knowledge cliff" when encountering real-world situations beyond their narrow representations[1][4][5].

Citations:
[1] https://journals.sagepub.com/doi/abs/10.1177/0306312793023003002
[2] https://www.jstor.org/stable/690004
[3] https://www.sup.org/books/anthropology/studying-those-who-study-us
[4] https://wiki.dickinson.edu/index.php/Forsythe1993
[5] https://journals.sagepub.com/doi/10.1177/016224399301800404
[6] https://www.degruyter.com/document/doi/10.1515/9781503619371-005/html?lang=en
[7] https://books.google.com/books/about/Studying_Those_Who_Study_Us.html?id=orNUzuFQeLgC
[8] https://www.cyberneticforests.com/ai-images-3
[9] https://dokumen.pub/studying-those-who-study-us-an-anthropologist-in-the-world-of-artificial-intelligence-9781503619371.html
[10] https://www.scirp.org/reference/referencespapers
[11] https://lumendatabase.org/notices/40128755

---
Answer from Perplexity: https://www.perplexity.ai/search/sts-ms-cat-core-26eOjT2rQjiRqIxKt6q3GA?utm_source=copy_output