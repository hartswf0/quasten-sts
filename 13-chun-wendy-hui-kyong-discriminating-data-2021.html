<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chun Wendy Hui Kyong Discriminating Data 2021</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>Chun Wendy Hui Kyong Discriminating Data 2021</h1>
        <!-- Optional: Add a link back to an index if needed -->
        <!-- <a href="index.html">Back to Index</a> -->
    </header>
    <main>
<h2>1. Problem Statement</h2>
<ol>
<li>Chun, Wendy Hui Kyong - Discriminating Data (2021)</li>
</ol>
<p>Focused Problem Statement: Presenting algorithmic hiring tools (L1 Machine Layer) as objective predictors of job success (L0 Reality) based on data patterns (L4 Data Positivism) obscures how their reliance on correlations within biased historical data actively reproduces discrimination. Chun's analysis (L3 Framing) models this as discriminatory correlation (homophily), revealing how seemingly neutral algorithms filter applicants in ways that reinforce past inequalities when interpreted by HR departments (L2).</p>
<h2>2. Core Model</h2>
<p><strong>13. Chun, Wendy Hui Kyong - Discriminating Data (2024)</strong></p>
<ul>
<li><strong>MODEL:</strong> Argues that contemporary data analysis and machine learning algorithms, far from being objective, are deeply embedded with historical biases and actively produce discrimination by correlating data in ways that entrench social divisions, particularly along lines of race and gender, under the guise of pattern recognition.</li>
<li><strong>Index:</strong><ul>
<li><strong>◻ Data Analysis / Machine Learning Algorithms</strong> → <strong>◻ Correlation / Pattern Recognition</strong></li>
<li><strong>◻ Correlation / Pattern Recognition</strong> → <strong>◻ (Claimed as) Objectivity</strong></li>
<li><strong>◻ Historical Bias / Social Divisions</strong> → <strong>◻ Embedded within Algorithms</strong></li>
<li><strong>◻ Algorithms</strong> → <strong>◻ Production/Entrenchment of Discrimination (esp. Race/Gender)</strong></li>
</ul>
</li>
</ul>
<h2>3. Four-Level Analysis</h2>
<div class="four-level-analysis">
<p><strong>13. Chun, Wendy Hui Kyong - Discriminating Data (2021)</strong></p>
<p>System: An algorithmic hiring tool using historical data to predict job success.</p>
<p>Level 0: <span class="angle-bracket-content">&lt;Reality&gt;</span> = <span class="angle-bracket-content">&lt;Complex Factors Contributing to Job Performance&gt;</span> &amp; <span class="angle-bracket-content">&lt;Historical Patterns of Workplace Discrimination&gt;</span></p>
<p>Level 1: <span class="angle-bracket-content">&lt;Machine Layer&gt;</span> = <span class="angle-bracket-content">&lt;Algorithmic Hiring Tool&gt;</span><br />
    <span class="angle-bracket-content">&lt;Hiring Tool&gt;</span> [simulates] → <span class="angle-bracket-content">&lt;Correlation between Past Employee Data &amp; Success Metrics&gt;</span> as <span class="angle-bracket-content">&lt;Predictive of Future Success&gt;</span><br />
    <span class="angle-bracket-content">&lt;Hiring Tool&gt;</span> [outputs] → <span class="angle-bracket-content">&lt;Ranked List of Candidates or Hire/No-Hire Recommendation&gt;</span><br />
    <span class="angle-bracket-content">&lt;Hiring Tool&gt;</span> [filters] → <span class="angle-bracket-content">&lt;Applicant Data&gt;</span> based on <span class="angle-bracket-content">&lt;Correlations found in Biased Historical Data&gt;</span></p>
<p>Level 2: <span class="angle-bracket-content">&lt;Human Use Layer&gt;</span> = <span class="angle-bracket-content">&lt;HR Department&gt;</span>, <span class="angle-bracket-content">&lt;System Developers&gt;</span><br />
    <span class="angle-bracket-content">&lt;HR Department&gt;</span> [interprets] → <span class="angle-bracket-content">&lt;Tool's Output&gt;</span> as <span class="angle-bracket-content">&lt;Objective/Efficient Basis for Decision&gt;</span><br />
    <span class="angle-bracket-content">&lt;System Developers&gt;</span> [steer] → <span class="angle-bracket-content">&lt;Algorithm's Focus&gt;</span> towards <span class="angle-bracket-content">&lt;Measurable Proxies for Success&gt;</span> (which may embed bias)</p>
<p>Level 3: <span class="angle-bracket-content">&lt;Reflexive Framing Layer&gt;</span> = <span class="angle-bracket-content">&lt;Chun (Critical Data Theorist)&gt;</span><br />
    <span class="angle-bracket-content">&lt;Chun&gt;</span> [models] → <span class="angle-bracket-content">&lt;Algorithmic Prediction&gt;</span> as <span class="angle-bracket-content">&lt;Enacting Discrimination through Correlation (Homophily)&gt;</span><br />
    <span class="angle-bracket-content">&lt;Chun&gt;</span> [challenges ontology] → <span class="angle-bracket-content">&lt;Of Algorithmic Objectivity&gt;</span>, revealing its grounding in past bias.<br />
    <span class="angle-bracket-content">&lt;Chun&gt;</span> [historicizes] → <span class="angle-bracket-content">&lt;Data Analytics&gt;</span> by linking <span class="angle-bracket-content">&lt;Correlation Logic to Histories of Eugenics/Segregation&gt;</span></p>
<p>Level 4: <span class="angle-bracket-content">&lt;Latent &amp; Systemic Evaluation&gt;</span> = <span class="angle-bracket-content">&lt;Ideology of Data Positivism &amp; Correlation / Neoliberal Logic of Efficiency&gt;</span><br />
    <span class="angle-bracket-content">&lt;Data Positivism&gt;</span> [naturalizes] → <span class="angle-bracket-content">&lt;Patterns in Data as Objective Truth / Correlation as Causation (effectively)&gt;</span><br />
    <span class="angle-bracket-content">&lt;Neoliberal Logic&gt;</span> [obscures] → <span class="angle-bracket-content">&lt;Ethical Costs &amp; Discriminatory Impacts&gt;</span> in favor of <span class="angle-bracket-content">&lt;Efficiency/Optimization&gt;</span><br />
    <span class="angle-bracket-content">&lt;System Logic&gt;</span> [reframes] → <span class="angle-bracket-content">&lt;Discrimination&gt;</span> as <span class="angle-bracket-content">&lt;Statistical Pattern Recognition&gt;</span></p>
</div>
<h2>4. Spatiotemporal Framework</h2>
<p><strong>13. Chun, Wendy Hui Kyong - Discriminating Data (2021)</strong></p>
<ul>
<li><strong>◻ SPATIOTEMPORAL UNIVERSE:</strong> Contemporary society increasingly reliant on data analytics, machine learning, and algorithmic decision-making.<ul>
<li><strong>→ [Contains] → ◻ FIELD:</strong> Practices and ideologies of data correlation, pattern recognition, and algorithmic prediction.<ul>
<li><strong>→ [Has] → ◻ DOMAIN:</strong> Large datasets, machine learning algorithms, computing infrastructure × Social structures (esp. race, gender, class).</li>
<li><strong>→ [Projects] → ◻ SIGNAL:</strong> Algorithmic outputs (predictions, classifications, recommendations), data visualizations, discourses about AI objectivity. (ℝᵏ representing correlation coefficients, classification labels, similarity scores).</li>
<li><strong>→ [Admits] → ◻ PERTURBATION:</strong> New algorithms, changes in data collection, critiques of bias, attempts at algorithmic fairness.</li>
</ul>
</li>
<li><strong>→ [Observed Through] → ◻ WINDOW:</strong> Critical data studies, media theory, science and technology studies (STS), critical race and gender theory.<ul>
<li><strong>→ [Defines] → ◻ OBSERVATION SCALE:</strong> Focus on the underlying logic of correlation vs. causation, the historical embedding of bias, the performativity of algorithms.</li>
<li><strong>→ [Delimits] → ◻ CONTEXT BOUNDARY:</strong> Critiques the claims of neutrality/objectivity in data science, connects current practices to histories of eugenics and segregation.</li>
<li><strong>→ [Generates] → ◻ SAMPLING LATTICE:</strong> Analysis of specific algorithms (e.g., facial recognition, predictive policing), datasets, discourses surrounding AI/ML, historical examples of discriminatory data use.</li>
</ul>
</li>
<li><strong>→ [Manifests As] → ◻ PHENOMENON:</strong> Correlation as Discrimination / Homophily as Control – Algorithms based on correlation inherently amplify past biases present in data, creating classifications and predictions that reinforce social divisions (homophily). This operates under a guise of objectivity but functions as a form of governance and discrimination.</li>
</ul>
</li>
<li><strong>◻ ENGINE:</strong> Critical Deconstruction of Algorithmic Logic.<ul>
<li><strong>→ [Applies] → ◻ TRANSFORM:</strong> Revealing the gap between correlation and causation, exposing the historical and social biases embedded in data/algorithms, analyzing the performative effects of algorithmic categorization.<ul>
<li><strong>→ [Operates On] → ◻ PATCH:</strong> Specific algorithmic processes, datasets, claims about data patterns.</li>
<li><strong>→ [Produces] → ◻ INVARIANT:</strong> The logic of correlation replacing causation; the principle of homophily driving division; the entanglement of data practices with historical discrimination.</li>
<li><strong>→ [Requires] → ◻ CONTRACTION:</strong> Focusing on the underlying mathematical/logical operations and their ideological implications, bracketing claims of mere technical optimization.</li>
</ul>
</li>
<li><strong>→ [Constructs] → ◻ REPRESENTATION:</strong> A theory of how data analytics actively produces discrimination through its reliance on correlation and its embedding of past inequalities.</li>
<li><strong>→ [Optimizes] → ◻ RECURSION:</strong> Applying the critique across various domains where algorithms are used (e.g., finance, policing, social media).<ul>
<li><strong>→ [Embeds] → ◻ MODEL:</strong> Theories of ideology, governmentality (Foucault), performativity, critical race/gender studies, STS.</li>
<li><strong>→ [Projects] → ◻ METRIC:</strong> Degree of discriminatory outcome, reliance on correlation vs. causation, historical resonance of bias (qualitative/structural).</li>
<li><strong>→ [Seeks] → ◻ FIXED POINT:</strong> Recognition of the inherent political and discriminatory nature of current data analytics paradigms.</li>
</ul>
</li>
</ul>
</li>
<li><strong>◻ METAMODEL:</strong> Critical Theory of Algorithmic Culture. Governs the analysis of data-driven systems. Binds Mathematical/Algorithmic Operations DOMAIN ↔ Social/Political Consequences (Discrimination, Governance) CODMAIN.</li>
<li><strong>Distinctions &amp; Crossings:</strong><ul>
<li><code>◻ MODEL (Algorithmic output/prediction) | ◻ REALITY (Complex social reality, individual nuance)</code> → <code>◻ MEASUREMENT ARTIFACT</code> (The classifications and correlations produced by the model are artifacts that actively reshape social reality by imposing categories).</li>
<li><code>◻ TIME-LIKE (Historical data feeding future predictions) | ◻ SPACE-LIKE (Algorithmic partitioning of populations/data space)</code>. The <code>CAUSAL CONE</code> is problematic: correlation projects past patterns (time) onto present/future decisions, creating discriminatory feedback loops.</li>
<li><code>◻ CONTINUOUS (Social spectrums, individual variation) | ◻ DISCRETE (Data points, classifications, categories)</code> → <code>◻ SAMPLING THRESHOLD</code> (The act of discretization and categorization itself is shown to be a site of discrimination).</li>
</ul>
</li>
<li><strong>Operations:</strong> <code>Observe</code> algorithmic systems and their outputs. <code>Encode</code> them not as neutral tools but as ideological apparatuses. <code>Convolve</code> datasets (<code>PATCH</code>) with biased algorithms (<code>KERNEL</code>) to produce discriminatory <code>FEATURE</code>s (e.g., risk scores). <code>Entangle</code> correlation with historical discrimination. Critiques the <code>Forget</code>ting of context and history in data science.</li>
</ul>

    </main>
    <footer>
        <!-- Optional: Add footer content if needed -->
        <!-- <p><a href="index.html">Back to Index</a></p> -->
    </footer>
</body>
</html>
